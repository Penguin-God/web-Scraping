웹 스크래핑 : 웹페이지에서 내가 원하는 부분만 가져오는 것
웹 크롤링 : 어떤 웹페이지의 허용범위 내에서 그 페이지의 링크를 따라가 모든 내용을 가져오는 것(웹 페이지마다 허용범위가 정해져 있음)


I. Xpath
Xpath : html문서에 있는 어떤 엘리먼트를 찾기 위한 경로를 여러 특징(ex : id, class)을 가지고 정의한 것
Xpath는 특정 태그의 정보(ex : 질병관리본부 감염자수)를 가져오거나 하는등 복잡한 코드에서 어떠한 행동을 하기위해 사용함 

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>랜덤영단어</title>
    <link rel="stylesheet" href="영단어.css">
    <script src="영단어.js"></script> 
</head>
<body>
    <div id="단어장">
        <input type="button" class = "button" value="영단어" onclick="randomword()">
        <table border="2" align="center"> 
            <tr>
                <td class="단어장" id="word">영단어</td>
                <td class="단어장" id="뜻">뜻</td>
            </tr>
        </table>
    </div>
</body>
</html>

만약 위 코드에서 영단어를 가져오고 싶으면 Xpath는
/html/body/div/table/tr/td[2]가 됨 (여기서 tr태그 안에 td태그가 2개 있는데 이때 tr은 td의 부모 td끼리는 형제관계임)
위에 주소를 줄이면 //*[@id="뜻"] 으로 줄일 수 있음
/ : 내가 위치한 곳에서 한단계 아래에 있는 엘리먼트
// : 지금 위치한 하위 엘리먼트들을 모두 찾아보는 것
* : 모든 태그에서 찾는 것 
@ : 속성을 찾는것 (calss, id, value, type)
웹페이지 Xpath는 개발자 도구를 켜서 경로클릭 후 우클릭 Xpath copy로 바로 얻을 수 있음, Xpath copy pull path는 Xpath를 길게 구함
웹페이지에서 아무 구성요소나 우클릭 후에 검사를 누르면 개발자도구가 켜지고 그 구성요소의 html코드로 이동함


II. 정규식
정규식 : 규칙을 가진 문자열을 표현하는 식
re.compile("원하는 형태") : 원하는 형태로 컴파일
macth() :  주어진 문자열의 처음부터 일치하는지 확인
search() : 주어진 문자열 중에 일치하는게 있는지 확인
findall() : 일치하는 모든것을 리스트 형태로 가져옴

. : 어떤 문자하나 (ca.e) : case, caae 등 .위치에 문자열이 있는 것
^ : 문자열의 시작 (^de) : de로 시작하는 것
$ : 문자열의 끝 : ($d) : d로 끝나는 것


III. User-Agent
User-Agent : 우리가 접속해서 부라우저가 웹페이지를 요청할 때 요청 정보에 따라서 데스크탑 모바일 구분, 언어 등 다른 웹페이지를 보여주는데 
크롤링과 같이 일반적인 정보가 아닌 상태로 접속하면 웹페이지 접속을 막을때가 있는데 이때 User-Agent를 변경하여 사람이 접속하는척 할수 있음


IV. 원하는 html문서정보 가져오기 
1. html문서정보 등을 가져오기 위해 사용하는 라이브러리
Requests : 빠르지만 동적인 웹 페이지(스크롤을 내리거나 클릭해야 하는 페이지)에서는 동적행동을 할 수 없음
주어진 url을 통해 받아온 html에 원하는 정보가 있을때 사용하면 좋음

Selenium : 느리고 메모리도 좀 먹지만 동적인 웹 페이지에서 클릭등의 행동 가능
주어진 url을 통해 받아온 html에 원하는 정보가 없고 클릭, 로그인 등의 행동을 해야 원하는 정보를 얻을 수 있을때 사용
크롬 버전에 맞는 chromedriver.exe가 반드시 있어야 동작함
페이지에 대한 로딩이 필요할 때는 로딩하는 시간동안 코드가 실행되면 의미가 없으므로 대기시간이 필요함
Headless Chrome를 사용해 페이지를 띄우지 않고 좀 더 빠르게 크롤링 가능(때때로 User-Agent설정 필요함)

2. BeautifulSoup4 
라이브러리를 통해 가져온 html문서에서 원하는 정보를 추출할 수 있게 해주는 라이브러리
함수 및 기능
find(조건) : 조건에 맞는 첫번째 element 가져옴
find_all(조건) : 조건에 맞는 모든 element를 리스트 형태로 가져옴
find_next_sibling : 다음 형제 찾기(맨뒤에 s붙이면 여러개 가져옴)
find_previous_sibling : 이전 형제 찾기(맨뒤에 s붙이면 여러개 가져옴) 
.get_text() : 텍스트를 가져옴
["href"] : 속성을 가져옴(저건 a태그의 href속성을 가져온 것)


V. 파일 다운로드 및 생성
1. 이미지 다운로드
with open("파일명", "wb") as f:
    f.write(res.content)  # 더자세한 정보는 BeautifulSoup4 -> 다음이미지 -> 다음_영화.py

2.csv생성
import csv
f = open(filename, "w", encoding = "utf-8-sig", newline = "")
encoding = "utf-8-sig"을 해야 한글이 안깨지고 newline = ""해야 쓸데없는 줄이 안생김


VI. 주의할 점(막 쓰면 안돼요)
1. 무분별한 웹 크롤링 / 스크래핑은 대상서버에 부하를 줌 -> IP차단 당할수도 있음

2. 크롤링한 데이터 사용 주의 -> 가져온 이미지, 텍스트 무단 활용 시 저작권 등 법적 제제당할 수 있음

3. 웹페이지 주소 뒤에 robots.txt를 붙이면 다른 페이지가 나오는데 Allow은 가져가는거 허용 Disallow는 비허용
법적 효력은 없지만 페이지 제작사에서 권고한 내용이므로 가급적이면 따르는게 좋음